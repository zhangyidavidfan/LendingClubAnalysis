---
title: "STAT 471 Final Project Code"
output: html_notebook
---

#Preliminary

```{r}
#Load in the packages
library(readr)
library(ggplot2)
library(dplyr)
library(fmsb)
library(sylcount)
library(leaps)
library(glmnet)
library(car)
library(rpart)
library(RColorBrewer)
library(rpart.plot)
library(caret)
library(bsts)
library(randomForest)
#Loading in the dataset
LoanStats_Cleaned = read_csv("Downloads/LoanStats_07_11_Clean.csv")
```

```{r}
#Checking Basic Summary
summary(LoanStats_Cleaned)
```

```{r}
#Basic Table of Categorical Variables
table(LoanStats_Cleaned$loan_status)
```

#The Story So Far

```{r}
#Percent Recovered if a person charges off
LoanStats_Cleaned %>% filter(loan_status == "Charged Off") %>% mutate(percent_paid = total_pymnt/loan_amnt) %>% summarize(mean(percent_paid)) # get the value left by doing loan amount minus all thats paid
```

```{r}
#Clean the date variable to use for visualization
LoanStats_Cleaned$issue_d_mod = as.Date(paste("01",LoanStats_Cleaned$issue_d), format = "%d %b %Y")
```

```{r}
#Extract average loan amount and total approved loan per year
base_stats = LoanStats_Cleaned %>% group_by(format(issue_d_mod,"%Y")) %>% summarize(avg_loan_amt = mean(loan_amnt), count = n())

colnames(base_stats) = c("year","ln_amt","cnt")

# 8% yearly increase in loan amounts
lm(log(ln_amt) ~ as.integer(year), data = base_stats)
```

```{r}
# 5000 yearly increase in total customer; 10000 from 2011-2012
lm(cnt ~ as.integer(year), data = base_stats)
```

```{r fig.height=2.5}
#Visualize Average Loan Amount Across year in different loan grades
ggplot(LoanStats_Cleaned %>% group_by(issue_d_mod,grade) %>% summarize(avg_loan_amt = mean(loan_amnt), count = n()), aes(x = issue_d_mod, y = avg_loan_amt, color = grade)) + geom_point(alpha = 0.2) + geom_smooth(se = F, size = 1.5,method = "loess") + theme_minimal() + scale_colour_manual(values = c("dark blue", "skyblue","gold","orange","orangered","red", "brown")) + theme(legend.position = "bottom",legend.box = "horizontal") + labs(title = "Loan Grade and Average Loan Amount",subtitle = "2008-2012",caption = "figure1: Lending Club Analysis") +xlab("Loan Issue Date") + ylab("Average Amount")
```

```{r}
#Visualize Maximum Loan Amount Across year in different loan grades
ggplot(LoanStats_Cleaned %>% group_by(issue_d_mod, grade) %>% summarize(max_loan_amt = max(loan_amnt)), aes(x = issue_d_mod, y = max_loan_amt, color = grade)) + geom_point(alpha = 0.2) + geom_smooth(se = F, size = 1.5,method = "loess") + theme_minimal() + scale_colour_manual(values = c("dark blue", "skyblue","gold","orange","orangered","red", "brown")) + theme(legend.position = "bottom",legend.box = "horizontal") + labs(title = "Loan Grade and Maximum Loan Amount",subtitle = "2008-2012",caption = "figure2: Lending Club Analysis") +xlab("Loan Issue Date") + ylab("Max Amount")
```

```{r}
#Visualize Number of loans Across year in different loan grades
ggplot(LoanStats_Cleaned %>% group_by(issue_d_mod, grade) %>% summarize(num_loan_approved = n()), aes(x = issue_d_mod, y = num_loan_approved, color = grade)) + geom_point(alpha = 0.2) + geom_smooth(se = F, size = 1.5,method = "loess") + theme_minimal() + scale_colour_manual(values = c("dark blue", "skyblue","gold","orange","orangered","red", "brown")) + theme(legend.position = "bottom",legend.box = "horizontal") + labs(title = "Loan Grade and Number of Loans",subtitle = "2008-2012",caption = "figure3: Lending Club Analysis") +xlab("Loan Issue Date") + ylab("Total Volume")
```

```{r}
#Visualize Default Rate Across year in different loan grades
ggplot(LoanStats_Cleaned %>% group_by(issue_d_mod, grade) %>% summarize(default_rate = sum(ifelse(loan_status == "Charged Off",1,0))/n()), aes(x = issue_d_mod, y = default_rate,color = grade)) + geom_point(alpha = 0.2) + geom_smooth(se = F, size = 1.5,method = "loess") + theme_minimal() + scale_colour_manual(values = c("dark blue", "skyblue","gold","orange","orangered","red", "brown")) + theme(legend.position = "bottom",legend.box = "horizontal") + labs(title = "Loan Grade and Default Rate",subtitle = "2008-2012",caption = "figure4: Lending Club Analysis") +xlab("Loan Issue Date") + ylab("Default")
```

```{r}
#What are the top recurring borrowers: relationships
LoanStats_Cleaned %>% group_by(emp_title) %>% summarize(count = n()) %>% arrange(desc(count))
```

```{r}
#Extarct the top brands Lending club partners with, EXCLUDE NA
top_brands = unlist(LoanStats_Cleaned %>% group_by(emp_title) %>% filter(!is.na(emp_title)) %>% summarize(count = n()) %>% arrange(desc(count)) %>% dplyr::select(emp_title) %>% head())
```

```{r}
##Visualize # of transaction with big brands Across year
LoanStats_Cleaned  %>% filter(emp_title %in% top_brands) %>% group_by(emp_title,issue_d_mod) %>% summarize(count = n()) %>% arrange(desc(count)) %>% ggplot(aes(color = emp_title, x = issue_d_mod, y = count)) + geom_point(alpha = 0.2) + geom_smooth(se = F, size = 1.5,method = "loess") + theme_minimal() + scale_colour_manual(values = c("skyblue", "red","black","darkgreen","lightgrey","orange")) + theme(legend.position = "bottom",legend.box = "horizontal") + labs(title = "Big Brand Client",subtitle = "2008-2012",caption = "figure5: Lending Club Analysis") +xlab("Loan Issue Date") + ylab("Loan Volume")
```

# The Dataset

```{r}
#irrelevant post loan data
Post_Loan_Data = c("issue_d","loan_status","funded_amnt","funded_amnt_inv","total_pymnt","total_pymnt_inv","total_rec_prncp","total_rec_int","total_rec_late_fee","recoveries", "collection_recovery_fee","last_pymnt_d", "last_pymnt_amnt","last_credit_pull_d")
```

```{r}
#Convert the credit line date to date
LoanStats_Cleaned$earliest_cr_line_mod =  as.Date(paste("01",LoanStats_Cleaned$earliest_cr_line), format = "%d %b %Y")
```

```{r}
#feature Engineering: humber of borrowing: see report for detail
LoanStats_Cleaned = LoanStats_Cleaned %>% group_by(emp_title) %>% arrange(emp_title,issue_d) %>% mutate("number_of_borrowing" = cumsum(grade == grade))
LoanStats_Cleaned$number_of_borrowing = ifelse(is.na(LoanStats_Cleaned$emp_title),0,LoanStats_Cleaned$number_of_borrowing)
```

```{r}
#Target Variable
LoanStats_Cleaned$Target = LoanStats_Cleaned$loan_status != "Fully Paid"
```

```{r}
#Extract Expected Return of good and bad loan investment
LoanStats_Cleaned$ExpectedReturn = ifelse(!LoanStats_Cleaned$Target,LoanStats_Cleaned$installment * as.integer(gsub(LoanStats_Cleaned$term, pattern = "_months",replacement = "")) - LoanStats_Cleaned$loan_amnt,LoanStats_Cleaned$total_pymnt-LoanStats_Cleaned$loan_amnt)
All_ER = LoanStats_Cleaned$ExpectedReturn
```

```{r}
#Get the median and mean value of expected return
LoanStats_Cleaned %>% group_by(Target) %>% summarize(median_expected_return = median(ExpectedReturn),mean_expected_return = mean(ExpectedReturn))
```

```{r}
#Get Risk Ratio
3581.15/1618.72
```

```{r}
#Filter Out Post Loan Data
LoanStats_Cleaned = LoanStats_Cleaned %>% dplyr::select(-Post_Loan_Data,-issue_d_mod)
```

```{r}
#Load in the full dataset and remove irrelevant variables
full = read_csv("Downloads/LoanStats_07_11_Full.csv")
summary(full$desc)
full_prune = full %>% dplyr::select(loan_amnt,zip_code,emp_title,annual_inc,desc,dti,total_rec_int)
```

```{r}
#Merged the full data onto the clean data
LoanStats_merged = merge(full_prune, LoanStats_Cleaned, by = c("loan_amnt","zip_code","emp_title","annual_inc","dti","total_rec_int"),all.y = T)
```

```{r}
#remove earliest credit line and emp title since one is repetitive while the other is too unique
LoanStats_Cleaned = LoanStats_Cleaned %>% ungroup() %>% dplyr::select(-earliest_cr_line,-emp_title)
#remove expected return
LoanStats_Cleaned = LoanStats_Cleaned %>% dplyr::select(-ExpectedReturn)
```

```{r}
#Constructing correlation plot for numeric variable
res1 <- corrplot::cor.mtest(LoanStats_Cleaned[,sapply(LoanStats_Cleaned, function(x) is.numeric(x) | is.logical(x))], conf.level = .95)
corrplot::corrplot(cor(LoanStats_Cleaned[,sapply(LoanStats_Cleaned, function(x) is.numeric(x) | is.logical(x))]), method = "color", tl.col = "black",tl.cex = 0.5, p.mat = res1$p,insig = "label_sig",
         sig.level = c(.001, .01, .05), pch.cex = .7, pch.col = "lightgrey",col=colorRampPalette(c("skyblue","white","orangered"))(200),title = "Loan Status Correlation Matrix") # add in significance label
```

```{r}
#Visualize number of borrowing and default rate
ggplot(data = LoanStats_Cleaned %>% group_by(number_of_borrowing) %>% summarize(mean_default = mean(Target),count = n()),aes(x = number_of_borrowing,y=mean_default,size = count)) + geom_point(color = "orange")  + theme_minimal() + theme(legend.position = "bottom",legend.box = "horizontal") + labs(title = "Loan Default Rate and Number of Borrowing",caption = "figure9: Lending Club Analysis") +xlab("Number of transaction with Lending Club") + ylab("Loan Default Rate")
```

```{r}
#Extarct all categorical data in the Loan dataset
Loan_categorical = (LoanStats_Cleaned %>% dplyr::select(-zip_code))[,colnames(LoanStats_Cleaned %>% dplyr::select(-zip_code))[!sapply(LoanStats_Cleaned %>% dplyr::select(-zip_code), function(x) is.numeric(x))]]
```

```{r message = F, fig.height=3.5}
#Visualize state and default rate
states <- map_data("state")
state_abbrev = read_csv("Downloads/STATE_ABBREV.csv") #load external data to extract state abbrevaition
colnames(state_abbrev) = c("STATE","ABBREV")
state_abbrev$STATE = tolower(state_abbrev$STATE)
states <- merge(states, state_abbrev, by.x = "region",by.y = "STATE",all.x = T)
region_loan = Loan_categorical %>% group_by(addr_state) %>% summarize(count = n(), mean.default = sum(Target)/n())
map <- merge(region_loan, states, sort=FALSE,by.x = "addr_state", by.y="ABBREV", all.y=TRUE)
map <- map[order(map$order),] #Re-establish the point order
map[is.na(map)] = 0

ggplot(map, aes(x=long, y=lat, group=group))+
geom_polygon(aes(alpha=count,fill = mean.default))+
geom_path() +
scale_fill_gradient(low = "gold",high = "orangered") + theme_minimal() + theme(legend.position = "bottom") + labs(title = "Loan Default Across States",caption = "figure6: Lending Club Analysis") +xlab("Longitude") + ylab("Latitude") + theme(title = element_text(size = 17))
```

```{r fig.height=3,fig.width=9}
#Visualize home ownership and default rate
ggplot(data = LoanStats_Cleaned %>% group_by(home_ownership) %>% summarize(mean_default_rate = mean(Target), count = n()), aes(x = home_ownership, y = mean_default_rate, size = count))  +
  geom_segment( aes(x=home_ownership, xend=home_ownership, y=0, yend=mean_default_rate),size = 0.5, color = "gold") +
  geom_point(color = "orange") + coord_flip() + theme_minimal() + theme(legend.position = "bottom",legend.box = "horizontal") + labs(title = "Loan Default Rate Across Home Ownership",caption = "figure7: Lending Club Analysis") +xlab("Home Ownership") + ylab("Loan Default Rate") + theme(title = element_text(size = 12))
```
```{r}
#Visualize purpose and default rate
label_data <- LoanStats_Cleaned %>% group_by(purpose) %>% summarize(mean_default_rate = mean(Target), count = n())
label_data$id = seq(1,nrow(label_data))
# calculate the ANGLE of the labels
number_of_bar <- nrow(label_data)
angle <-  90 - 360 * (label_data$id-0.5) /number_of_bar     # I substract 0.5 because the letter must have the angle of the center of the bars. Not extreme right(1) or extreme left (0)
 
# calculate the alignment of labels: right or left
# If I am on the left part of the plot, my labels have currently an angle < -90
label_data$hjust<-ifelse( angle < -90, 1, 0)
 
# flip angle BY to make them readable
label_data$angle<-ifelse(angle < -90, angle+180, angle)

ggplot(data = LoanStats_Cleaned %>% group_by(purpose) %>% summarize(mean_default_rate = mean(Target), count = n()), aes(x = purpose, y = mean_default_rate, alpha = count))  +
  geom_col(fill = "orange")  + coord_polar(start = 0) + theme_minimal() +theme(
    axis.text = element_blank(),
    axis.title = element_blank(),
    panel.grid = element_blank(),
    plot.margin = unit(rep(-1,4), "cm")      # Adjust the margin to make in sort labels are not truncated!
  ) + geom_text(data=label_data, aes(x=id, y=mean_default_rate+0.05, label=paste(purpose,"\n",scales::percent(mean_default_rate)), hjust=hjust), color="black", fontface="bold",alpha=0.6, size=2.5, angle= label_data$angle, inherit.aes = FALSE ) + ylim(-0.2,0.35)
```
```{r}
#Visualize employment length and default rate
Emp_loan_count = LoanStats_Cleaned %>% group_by(emp_length) %>% summarize(mean_default_rate = mean(Target), count = n())
Emp_loan_count$year = c(0,1,10,2,3,4,5,6,7,8,9,NA)
Emp_loan_count$group = 1
ggplot(data = Emp_loan_count %>% arrange(year), aes(x = reorder(emp_length,year), y = mean_default_rate,group = group,size = count)) + geom_line(color = "gold", size = 1) + geom_point(color = "orange")  + theme_minimal() + theme(legend.position = "bottom",legend.box = "horizontal") + labs(title = "Loan Default Rate Across Employment Length",caption = "figure8: Lending Club Analysis") +xlab("Employment Length") + ylab("Loan Default Rate")
```

```{r}
#Feature Enginner number of words and smog score
num_words = sapply(1:nrow(LoanStats_merged), function(x)
readability(LoanStats_merged$desc[x])$words)
smog_score = sapply(1:nrow(LoanStats_merged), function(x)
readability(LoanStats_merged$desc[x])$smog)
```

```{r}
#Sort the dataframes in the same way 
LoanStats_merged = LoanStats_merged %>% arrange(loan_amnt,zip_code,annual_inc,dti)
LoanStats_Cleaned = LoanStats_Cleaned %>% arrange(loan_amnt,zip_code,annual_inc,dti)
#migrate value from one dataframe to the other
LoanStats_Cleaned$num_words = num_words
LoanStats_Cleaned$smog_score = smog_score
```


```{r}
#Visualize smog score
LoanStats_Cleaned %>% group_by(round_smog = round(smog_score,0)) %>% summarize(count = n(), mean_default = mean(Target)) %>% mutate(group= 1) %>% ggplot(aes(x = round_smog,y=mean_default,size = count, group = group))  + geom_point(color = "orange")  + theme_minimal() + theme(legend.position = "bottom",legend.box = "horizontal") + labs(title = "Loan Default Rate and Fog Score",caption = "figure9: Lending Club Analysis") +xlab("Fog Score") + ylab("Loan Default Rate")
```


```{r}
#Clean the smog score to engineer a separate variable if smog score is undefined/ set the undefined smog score to be 0
LoanStats_Cleaned$undefined_smog_score = is.na(LoanStats_Cleaned$smog_score) | LoanStats_Cleaned$smog_score == Inf
LoanStats_Cleaned$smog_score = ifelse(is.na(LoanStats_Cleaned$smog_score) | LoanStats_Cleaned$smog_score == Inf, 0, LoanStats_Cleaned$smog_score)
```

# Modelling

```{r}
# Create the weighted misclassification function
MCE = function(prediction,actual){
  False_negative = sum(prediction[actual == TRUE] != TRUE)
  False_positive = sum(prediction[actual != TRUE] == TRUE)
  return((False_negative*2.2 + False_positive)/length(actual))
}
```

```{r}
#Train test split
set.seed(123)
LoanStats_Use = LoanStats_Cleaned %>% dplyr::select(-sub_grade,-zip_code)
test_index = sample(1:38971,3897)
LoanStats_test = LoanStats_Use[test_index,]
LoanStats_train = LoanStats_Use[-test_index,]
```

```{r}
#Baseline Model predicting all as would not default
MCE(rep(FALSE,3897),
    LoanStats_test$Target)
```
```{r}
pROC::auc(pROC::roc(as.integer(LoanStats_test$Target),rep(0,3897)))
```

```{r}
#Regular Table
table(LoanStats_train$Target)
```

```{r warning = F}
#Model 1: Logistic Regression on all varaibles
mod1 = glm(data = LoanStats_train,Target~.,family = "binomial")
```

```{r}
#Logistic Regression Summmary
summary(mod1)
```

```{r}
#Confusion Matrix
table(predict(mod1,LoanStats_test,type = "response") > (1/2.2)/(1+(1/2.2)),
    LoanStats_test$Target)
```

```{r}
#Calculate MCE
MCE(predict(mod1,LoanStats_test,type = "response") > (1/2.2)/(1+(1/2.2)),
    LoanStats_test$Target)
```

```{r}
mod0_ROC = pROC::roc(as.integer(LoanStats_test$Target), predict(mod1,LoanStats_test,type = "response"))
pROC::auc(mod0_ROC)
```

```{r}
#Model2: Lasso
set.seed(123)
X = model.matrix(Target~.,LoanStats_Use)[-test_index,]
Y = as.matrix(LoanStats_train$Target)
lasso = cv.glmnet(X,Y,family = "binomial")
plot(lasso)
```

```{r}
#Getting lambda min
lasso$lambda.min
```

```{r}
#Using lasso min raw model
MCE(predict(lasso$glmnet.fit,s = 0.00115396,model.matrix(Target~.,LoanStats_Use)[test_index,],type = "response") > (1/2.2)/(1+(1/2.2)),
    LoanStats_test$Target)
```

```{r}
pure_lasso_min_ROC = pROC::roc(as.integer(LoanStats_test$Target), predict(lasso$glmnet.fit,s = 0.00115396,model.matrix(Target~.,LoanStats_Use)[test_index,],type = "response"))
pROC::auc(pure_lasso_min_ROC)
```


```{r}
#Confusion Matrix
table(predict(lasso$glmnet.fit,s = 0.00115396,model.matrix(Target~.,LoanStats_Use)[test_index,],type = "response") > (1/2.2)/(1+(1/2.2)),
    LoanStats_test$Target)
```

```{r}
#Getting lambda 1se
lasso$lambda.1se
```

```{r}
#Using lasso 1se raw model
MCE(predict(lasso$glmnet.fit,s = 0.007417739,model.matrix(Target~.,LoanStats_Use)[test_index,],type = "response") > (1/2.2)/(1+(1/2.2)),
    LoanStats_test$Target)
```

```{r}
pure_lasso_1se_ROC = pROC::roc(as.integer(LoanStats_test$Target), predict(lasso$glmnet.fit,s = 0.007417739,model.matrix(Target~.,LoanStats_Use)[test_index,],type = "response"))
pROC::auc(pure_lasso_1se_ROC)
```

```{r}
#Confusion Matrix
table(predict(lasso$glmnet.fit,s = 0.007417739,model.matrix(Target~.,LoanStats_Use)[test_index,],type = "response") > (1/2.2)/(1+(1/2.2)),
    LoanStats_test$Target)
```

```{r}
#Model 2.A Lasso 1se
rownames(coef(lasso,s = "lambda.1se"))[which(coef(lasso,s = "lambda.1se") != 0)]
```

```{r}
#Model 2.B Lasso min
rownames(coef(lasso,s = "lambda.min"))[which(coef(lasso,s = "lambda.min") != 0)]
```

```{r}
#Build out relaxed lasso 1se
lasso_glm = glm(data = LoanStats_train, Target~term+int_rate+emp_length+annual_inc+purpose+addr_state+inq_last_6mths+pub_rec+revol_util+pub_rec_bankruptcies, family = binomial)
```

```{r}
#Build out relaxed lasso min
lasso_min_glm = glm(data = LoanStats_train, Target~loan_amnt + term+int_rate+grade+emp_length+home_ownership+dti+annual_inc+purpose+addr_state+inq_last_6mths+open_acc+pub_rec+revol_bal+revol_util+pub_rec_bankruptcies + earliest_cr_line_mod + number_of_borrowing + undefined_smog_score, family = binomial)
```

```{r warning = F}
#Backward selection lasso 1se
Anova(lasso_glm)
```

```{r warning = F}
#Backward selection lasso 1se - complete
lasso_glm2 = update(lasso_glm,.~.-pub_rec_bankruptcies)
Anova(lasso_glm2)
```

```{r}
#MCE lasso 1se final model
MCE(predict(lasso_glm2,LoanStats_test,type = "response") > (1/2.2)/(1+(1/2.2)),
    LoanStats_test$Target)
```

```{r}
#Confusion Matrix
table(predict(lasso_glm2,LoanStats_test,type = "response") > (1/2.2)/(1+(1/2.2)),
    LoanStats_test$Target)
```

```{r}
lasso_1se_ROC = pROC::roc(as.integer(LoanStats_test$Target), predict(lasso_glm2,LoanStats_test,type = "response"))
pROC::auc(lasso_1se_ROC)
```

```{r warning = F}
#Backward selection lasso min
Anova(lasso_min_glm)
```

```{r warning = False}
#Backward selection lasso min
lasso_min_glm2 = update(lasso_min_glm,.~.-earliest_cr_line_mod)
Anova(lasso_min_glm2)
```

```{r warning = False}
#Backward selection lasso min
lasso_min_glm3 = update(lasso_min_glm2,.~.-dti)
Anova(lasso_min_glm3)
```

```{r warning = False}
#Backward selection lasso min
lasso_min_glm4 = update(lasso_min_glm3,.~.-pub_rec_bankruptcies)
Anova(lasso_min_glm4)
```

```{r warning = False}
#Backward selection lasso min
lasso_min_glm5 = update(lasso_min_glm4,.~.-number_of_borrowing)
Anova(lasso_min_glm5)
```

```{r warning = False}
#Backward selection lasso min
lasso_min_glm6 = update(lasso_min_glm5,.~.-loan_amnt)
Anova(lasso_min_glm6)
```
 
```{r warning = False}
#Backward selection lasso min
lasso_min_glm7 = update(lasso_min_glm6,.~.-undefined_smog_score)
Anova(lasso_min_glm7)
```

```{r warning = F}
#Backward selection lasso min - complete
lasso_min_glm8 = update(lasso_min_glm7,.~.-open_acc)
Anova(lasso_min_glm8)
```

```{r}
#Final MCE lasso min
MCE(predict(lasso_min_glm8,LoanStats_test,type = "response") > (1/2.2)/(1+(1/2.2)),
    LoanStats_test$Target)
```

```{r}
#Confusion Matrix
table(predict(lasso_min_glm8,LoanStats_test,type = "response") > (1/2.2)/(1+(1/2.2)),
    LoanStats_test$Target)
```

```{r}
lasso_min_ROC = pROC::roc(as.integer(LoanStats_test$Target), predict(lasso_min_glm8,LoanStats_test,type = "response"))
pROC::auc(lasso_min_ROC)
```

```{r}
#Model3: Ridge
set.seed(123)
X = model.matrix(Target~.,LoanStats_Use)[-test_index,]
Y = as.matrix(LoanStats_train$Target)
ridge= cv.glmnet(X,Y,family = "binomial",alpha = 0)
plot(ridge)
```

```{r}
#Getting lambda min
ridge$lambda.min
```

```{r}
#Using ridge min raw model
MCE(predict(ridge$glmnet.fit,s = ridge$lambda.min,model.matrix(Target~.,LoanStats_Use)[test_index,],type = "response") > (1/2.2)/(1+(1/2.2)),
    LoanStats_test$Target)
```

```{r}
#Confusion Matrix
table(predict(ridge$glmnet.fit,s = ridge$lambda.min,model.matrix(Target~.,LoanStats_Use)[test_index,],type = "response") > (1/2.2)/(1+(1/2.2)),
    LoanStats_test$Target)
```

```{r}
ridge1_ROC = pROC::roc(as.integer(LoanStats_test$Target), predict(ridge$glmnet.fit,s = ridge$lambda.min,model.matrix(Target~.,LoanStats_Use)[test_index,],type = "response"))
pROC::auc(ridge1_ROC)
```

```{r}
#Getting lambda 1se
ridge$lambda.1se
```

```{r}
#Using ridge 1se raw model
MCE(predict(ridge$glmnet.fit,s = ridge$lambda.1se,model.matrix(Target~.,LoanStats_Use)[test_index,],type = "response") > (1/2.2)/(1+(1/2.2)),
    LoanStats_test$Target)
```

```{r}
#Confusion Matrix
table(predict(ridge$glmnet.fit,s = ridge$lambda.1se,model.matrix(Target~.,LoanStats_Use)[test_index,],type = "response") > (1/2.2)/(1+(1/2.2)),
    LoanStats_test$Target)
```

```{r}
ridge2_ROC = pROC::roc(as.integer(LoanStats_test$Target), predict(ridge$glmnet.fit,s = ridge$lambda.1se,model.matrix(Target~.,LoanStats_Use)[test_index,],type = "response"))
pROC::auc(ridge2_ROC)
```

```{r}
#Model4: Elastic Net
set.seed(123)
X = model.matrix(Target~.,LoanStats_Use)[-test_index,]
Y = as.matrix(LoanStats_train$Target)

sapply(c(0.3,0.5,0.7), function(x){
  print(paste("alpha value",x))
  elastic= cv.glmnet(X,Y,family = "binomial",alpha = x)
  #Using elastic min raw model
  print(MCE(predict(elastic$glmnet.fit,s = elastic$lambda.min,model.matrix(Target~.,LoanStats_Use)[test_index,],type = "response") > (1/2.2)/(1+(1/2.2)),
      LoanStats_test$Target))
  
  #AUC ROC
  el_ROC = pROC::roc(as.integer(LoanStats_test$Target), predict(elastic$glmnet.fit,s = elastic$lambda.min,model.matrix(Target~.,LoanStats_Use)[test_index,],type = "response"))
  print(pROC::auc(el_ROC))
  
  #Using elastic 1se raw model
  print(MCE(predict(elastic$glmnet.fit,s = elastic$lambda.1se,model.matrix(Target~.,LoanStats_Use)[test_index,],type = "response") > (1/2.2)/(1+(1/2.2)),
      LoanStats_test$Target))
  
  #AUC ROC
  el2_ROC = pROC::roc(as.integer(LoanStats_test$Target), predict(elastic$glmnet.fit,s = elastic$lambda.1se,model.matrix(Target~.,LoanStats_Use)[test_index,],type = "response"))
  print(pROC::auc(el2_ROC))
}
)
```

```{r}
set.seed(123)
elastic_fin= cv.glmnet(X,Y,family = "binomial",alpha = 0.3)
plot(elastic_fin)
#Using elastic 1se raw model
print(MCE(predict(elastic_fin$glmnet.fit,s = elastic_fin$lambda.1se,model.matrix(Target~.,LoanStats_Use)[test_index,],type = "response") > (1/2.2)/(1+(1/2.2)),
      LoanStats_test$Target))
  
  #Confusion Matrix
table(predict(elastic_fin$glmnet.fit,s = elastic_fin$lambda.1se,model.matrix(Target~.,LoanStats_Use)[test_index,],type = "response") > (1/2.2)/(1+(1/2.2)),
      LoanStats_test$Target)
```

```{r}
(488 * 2.2 + 56)/nrow(LoanStats_test)
```

```{r}
#Create elaborate MCE function for CARET
MCE_fun = function(data, lev = NULL, model = NULL){
  print(colnames(data))
  actual = data$obs == "default"
  prediction = data$pred == "default"
  prediction2 = data$default > (1/2.2)/(1+(1/2.2))
  False_negative = sum(prediction[actual == TRUE] != TRUE)
  False_positive = sum(prediction[actual != TRUE] == TRUE)
  False_negative2 = sum(prediction2[actual == TRUE] != TRUE)
  False_positive2 = sum(prediction2[actual != TRUE] == TRUE)
  weighted_mce = (False_negative*2.2 + False_positive)/length(actual)
  weighted_mce_bayes = (False_negative2*2.2 + False_positive2)/length(actual)
  percent_postive_pred = sum(prediction2)/length(actual)
  both = c(weighted_mce,weighted_mce_bayes,percent_postive_pred)
  names(both) = c("weighted_mce","weighted_mce_bayes","percent_positive_pred")
  return(both)
}
```

```{r}
#Model5: Rpart
#Hyperparamter tune rpart: CP
ctrl <- trainControl(method = "repeatedcv",
                     number = 10,
                     repeats = 1,
                     classProbs = TRUE,
                     summaryFunction = MCE_fun)

tune.gridcart <- expand.grid(cp = GeometricSequence(6,0.00096,0.85))

#LoanStats_train$Target = as.factor(LoanStats_train$Target)
LoanStats_train2 = LoanStats_train
LoanStats_train2$Target = as.factor(ifelse(as.logical(LoanStats_train2$Target),"default","good"))

model <- train(Target~.,
               data = LoanStats_train2, 
               tuneGrid =tune.gridcart,
               method = "rpart",
               trControl = ctrl,
               maximize=FALSE,
               metric = "weighted_mce")

model
```

```{r}
#Fit rpart model based on tuned paramter
tree_model = rpart(Target~., LoanStats_train, method = "class",cp =0.00096)
```

```{r}
#tree model
rpart.plot(tree_model)
```

```{r}
#output base MCE
MCE(predict(tree_model,newdata = LoanStats_test,type = "class"),LoanStats_test$Target)
```

```{r}
#output MCE with base rule
MCE(predict(tree_model,newdata = LoanStats_test,type = "prob")[,2] > (1/2.2)/(1+(1/2.2)),LoanStats_test$Target)
```

```{r}
tree_ROC = pROC::roc(as.integer(LoanStats_test$Target), predict(tree_model,newdata = LoanStats_test,type = "prob")[,2] )
print(pROC::auc(tree_ROC))
```

```{r}
#Model4: RF
#Hyperparamter tune RF
# ctrl <- trainControl(method = "repeatedcv",
#                      number = 4,
#                      repeats = 1,
#                      classProbs = TRUE,
#                      summaryFunction = MCE_fun)
# 
# tunegrid <- expand.grid(mtry=c(1:15), ntree=c(1000, 1500, 2000, 2500))
# 
# #LoanStats_train$Target = as.factor(LoanStats_train$Target)
# LoanStats_train2 = LoanStats_train
# LoanStats_train2$Target = as.factor(ifelse(as.logical(LoanStats_train2$Target),"default","good"))
# 
# customRF <- list(type = "Classification", library = "randomForest", loop = NULL)
# customRF$parameters <- data.frame(parameter = c("mtry", "ntree"), class = rep("numeric", 2), label = c("mtry", "ntree"))
# customRF$grid <- function(x, y, len = NULL, search = "grid") {}
# customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
#   randomForest(x, y, mtry = param$mtry, ntree=param$ntree, ...)
# }
# customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
#    predict(modelFit, newdata)
# customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
#    predict(modelFit, newdata, type = "prob")
# customRF$sort <- function(x) x[order(x[,1]),]
# customRF$levels <- function(x) x$classes
# 
# model <- train(Target~.,
#                data = LoanStats_train2, 
#                tuneGrid =tunegrid,
#                method = customRF,
#                trControl = ctrl,
#                metric = "weighted_mce")
# 
# model
```

```{r}
#convert all character to factor for RF
# LoanStats_train = as.data.frame(unclass(LoanStats_train))
```

```{r}
#Fit rf model based on tuned paramter
# rf_mod1 = randomForest::randomForest(Target~., LoanStats_train, method = "class", mtry = 5)
```
```{r}
#output base MCE
# MCE(predict(rf_mod1,newdata = LoanStats_test,type = "class"),LoanStats_test$Target)
```

```{r}
#output MCE with bayes rule
# MCE(predict(rf_mod1,newdata = LoanStats_test,type = "prob")[,1] > (1/2.2)/(1+(1/2.2)),LoanStats_test$Target)
```


```{r}
#Attempt Upsampling
LoanStats_train_up <- upSample(x = LoanStats_train %>% dplyr::select(-Target),
                     y = as.factor(LoanStats_train$Target))                         
```


```{r}
#Model5: Rpart Upsampled
#Hyperparamter tuning: CP
ctrl <- trainControl(method = "repeatedcv",
                     number = 10,
                     repeats = 1,
                     classProbs = TRUE,
                     summaryFunction = MCE_fun)

tune.gridcart <- expand.grid(cp = GeometricSequence(5,0.0016,0.1))

#LoanStats_train$Target = as.factor(LoanStats_train$Target)
LoanStats_train_up2 = LoanStats_train_up
LoanStats_train_up2$Class = as.factor(ifelse(as.logical(LoanStats_train_up2$Class),"default","good"))

model <- train(Class~.,
               data = LoanStats_train_up2, 
               tuneGrid =tune.gridcart,
               method = "rpart",
               trControl = ctrl,
               maximize=FALSE,
               metric = "weighted_mce")

model
```

```{r}
#Fit rpart model based on tuned paramter
tree_model2 = rpart(Class~., LoanStats_train_up, method = "class",cp = 1.6e-05)
```

```{r}
#Extract variable importance based on tuned parameter
tree_model2$variable.importance
```

```{r}
#Onbtain MCE
MCE(predict(tree_model2,newdata = LoanStats_test,type = "class"),LoanStats_test$Target)
```

```{r}
tree2_ROC = pROC::roc(as.integer(LoanStats_test$Target), predict(tree_model2,newdata = LoanStats_test,type = "prob")[,2])
print(pROC::auc(tree2_ROC))
```

#Final model summary
```{r}
data.frame("Variables" = rownames(coef(elastic_fin$glmnet.fit, s = elastic_fin$lambda.1se))[which(coef(elastic_fin$glmnet.fit, s = elastic_fin$lambda.1se)!=0)], coefficients = coef(elastic_fin$glmnet.fit, s = elastic_fin$lambda.1se)[which(coef(elastic_fin$glmnet.fit, s = elastic_fin$lambda.1se)!=0)])
```

